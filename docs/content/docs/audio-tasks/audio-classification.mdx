---
title: Audio Classification (Android Only)
description: Perform classification on audio data.
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';
import { File, Folder } from 'fumadocs-ui/components/files';

The `useAudioClassifier` hook lets you perform classification on audio data. You can use this hook to identify sound events from a set of trained categories.

## Usage

Here is a basic example of how to use the `useAudioClassifier` hook:

```tsx
import { useAudioClassifier, type AudioClassifierResult } from 'react-native-aistack';
import { View, Button, Text } from 'react-native';

const MyComponent = () => {
  const { classify, isLoading, error } = useAudioClassifier(
    { uri: 'https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite' },
    { maxResults: 5 }
  );

  const handleClassify = async () => {
    try {
      const result:AudioClassifierResult = await classify({ uri: 'https://your-audio-url.com/audio.mp3' });
      console.log('Classification result:', result.classificationResults); // Access classificationResults
      if (result.classificationResults.length > 0) {
        console.log('First classification:', result.classificationResults[0].classifications); // Access classifications
      }
    } catch (e) {
      console.error('Classification failed:', e);
    }
  };

  return (
    <View>
      <Button
        title="Classify Audio"
        onPress={handleClassify}
        disabled={isLoading}
      />
      {isLoading && <Text>Loading model...</Text>}
      {error && <Text>Error: {error.message}</Text>}
    </View>
  );
};
```

## Model

The `useAudioClassifier` hook requires a trained audio classification model in TFLite format.

### Model Source

You can provide the model using one of the following sources:

- **URI:** The model is hosted on a remote server.
- **File Path:** The model is stored on the device's file system.
- **Bundle:** The model is included in your app's assets.

```ts
// From a remote URL
const modelSource = { uri: 'https://example.com/model.tflite' };

// From a local file path
const modelSource = { filePath: '/path/to/your/model.tflite' };

// From the app bundle
const modelSource = { bundle: 'yamnet.tflite' };
```

For more information on available models, see the [MediaPipe Audio Classifier models documentation](https://ai.google.dev/edge/mediapipe/solutions/audio/audio_classifier/index#models).

## Configuration

You can configure the audio classifier with the following options:

### `AudioClassifierOptions`

| Option               | Description                                                                 | Type       | Default       |
| -------------------- | --------------------------------------------------------------------------- | ---------- | ------------- |
| `runningMode`        | The running mode for the task. Only `AUDIO_CLIPS` mode is supported.        | `string`   | `AUDIO_CLIPS` |
| `maxResults`         | The maximum number of top-scored classification results to return.          | `number`   | `-1`          |
| `scoreThreshold`     | The prediction score threshold. Results below this value are rejected.      | `number`   | `0.0`         |
| `categoryAllowlist`  | An optional list of allowed category names.                                 | `string[]` | `[]`          |
| `categoryDenylist`   | An optional list of denied category names.                                  | `string[]` | `[]`          |
| `displayNamesLocale` | The locale to use for display names specified in the TFLite Model Metadata. | `string`   | `'en'`        |

### `AudioPreprocessingOptions`

| Option      | Description                                | Type     | Default |
| ----------- | ------------------------------------------ | -------- | ------- |
| `sampleRate`| The sample rate of the audio data.         | `number` | `16000` |

## Result

The `classify` function returns an `AudioClassifierResult` object with the following properties:

### `AudioClassifierResult`

| Property            | Description                                                                    | Type                 |
| ------------------- | ------------------------------------------------------------------------------ | -------------------- |
| `classificationResults` | A list of classification results, each potentially with its own timestamp.   | `ClassificationResult[]` |
| `inferenceTime`     | The inference time in milliseconds.                                            | `number`             |

### `ClassificationResult`

| Property       | Description                                       | Type     |
| -------------- | ------------------------------------------------- | -------- |
| `timestampMs`  | Optional timestamp in milliseconds for this specific classification result. | `number` |
| `classifications` | A list of classification groups for this result. | `Classifications[]` |

### `Classifications`

| Property       | Description                                       | Type     |
| -------------- | ------------------------------------------------- | -------- |
| `categories`   | The array of predicted categories, usually sorted by descending scores, e.g., from high to low probability. | `Category[]` |
| `headIndex`    | The index of the classifier head these categories refer to. | `number` |
| `headName`     | The name of the classifier head, which is the corresponding tensor metadata name. | `string` |

### `Category`

| Property       | Description                                       | Type     |
| -------------- | ------------------------------------------------- | -------- |
| `index`        | The index of the category in the model outputs.   | `number` |
| `score`        | The confidence score for this category.           | `number` |
| `displayName`  | The display name for the category.                | `string` |
| `categoryName` | The name of the category from the model metadata. | `string` |
